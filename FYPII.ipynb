{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYPII.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VREpXDE-MXWT"
      },
      "source": [
        "### Installations\n",
        "Required only once to run on the device (laptop)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5llZbG2MXWe"
      },
      "source": [
        "#pip install tensorflow==1.14.0\n",
        "#pip install q keras==2.1.5\n",
        "#pip install -U numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g342OHv1fnb"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-aulkTytkAV",
        "outputId": "8e2b947c-624c-40b8-9513-8dab5fa7a133"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "from tensorflow.python import keras\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhFr_XsFMXWq"
      },
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGdZMXzjxyUT"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import cv2\n",
        "import socket\n",
        "import json\n",
        "import threading\n",
        "from io import StringIO\n",
        "from _thread import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeGDLgH37NCZ"
      },
      "source": [
        "### Processing Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FSmVWSScYv6"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# REFERENCE: qqwweee github repository\n",
        "\"\"\"\n",
        "Class definition of YOLO_v3 style detection model on image and video\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from PIL import Image\n",
        "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
        "from yolo3.utils import letterbox_image\n",
        "import os\n",
        "from keras.utils import multi_gpu_model\n",
        "\n",
        "class YOLO(object):\n",
        "    _defaults = {\n",
        "        \"model_path\": 'trained_weights_final.h5',\n",
        "        \"anchors_path\": 'yolo3/yolo_anchors.txt',\n",
        "        \"classes_path\": 'classes.txt',\n",
        "        \"score\" : 0.3,\n",
        "        \"iou\" : 0.45,\n",
        "        \"model_image_size\" : (416, 416),\n",
        "        \"gpu_num\" : 1,\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def get_defaults(cls, n):\n",
        "        if n in cls._defaults:\n",
        "            return cls._defaults[n]\n",
        "        else:\n",
        "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(self._defaults) # set up default values\n",
        "        self.__dict__.update(kwargs) # and update with user overrides\n",
        "        self.class_names = self._get_class()\n",
        "        self.anchors = self._get_anchors()\n",
        "        self.sess = K.get_session()\n",
        "        self.boxes, self.scores, self.classes = self.generate()\n",
        "\n",
        "    def _get_class(self):\n",
        "        classes_path = os.path.expanduser(self.classes_path)\n",
        "        with open(classes_path) as f:\n",
        "            class_names = f.readlines()\n",
        "        class_names = [c.strip() for c in class_names]\n",
        "        return class_names\n",
        "\n",
        "    def _get_anchors(self):\n",
        "        anchors_path = os.path.expanduser(self.anchors_path)\n",
        "        with open(anchors_path) as f:\n",
        "            anchors = f.readline()\n",
        "        anchors = [float(x) for x in anchors.split(',')]\n",
        "        return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "    def generate(self):\n",
        "        model_path = os.path.expanduser(self.model_path)\n",
        "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
        "\n",
        "        # Load model, or construct model and load weights.\n",
        "        num_anchors = len(self.anchors)\n",
        "        num_classes = len(self.class_names)\n",
        "        is_tiny_version = num_anchors==6 # default setting\n",
        "        try:\n",
        "            self.yolo_model = load_model(model_path, compile=False)\n",
        "        except:\n",
        "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
        "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
        "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
        "        else:\n",
        "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
        "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
        "                'Mismatch between model and given anchor and class sizes'\n",
        "\n",
        "        # Generate output tensor targets for filtered bounding boxes.\n",
        "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
        "        if self.gpu_num>=2:\n",
        "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
        "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
        "                len(self.class_names), self.input_image_shape,\n",
        "                score_threshold=self.score, iou_threshold=self.iou)\n",
        "        return boxes, scores, classes\n",
        "\n",
        "    def detect_image(self):\n",
        "        \n",
        "        if self.model_image_size != (None, None):\n",
        "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
        "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
        "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
        "        else:\n",
        "            new_image_size = (image.width - (image.width % 32),\n",
        "                              image.height - (image.height % 32))\n",
        "            boxed_image = letterbox_image(image, new_image_size)\n",
        "        image_data = np.array(boxed_image, dtype='float32')\n",
        "\n",
        "        image_data /= 255.\n",
        "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "\n",
        "        out_boxes, out_scores, out_classes = self.sess.run(\n",
        "            [self.boxes, self.scores, self.classes],\n",
        "            feed_dict={\n",
        "                self.yolo_model.input: image_data,\n",
        "                self.input_image_shape: [image.size[1], image.size[0]],\n",
        "                K.learning_phase(): 0\n",
        "            })\n",
        "\n",
        "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
        "        \n",
        "        thickness = (image.size[0] + image.size[1]) // 300\n",
        "        for i, c in reversed(list(enumerate(out_classes))):\n",
        "            predicted_class = self.class_names[c]\n",
        "            box = out_boxes[i]\n",
        "            score = out_scores[i]\n",
        "            label = '{} {:.2f}'.format(predicted_class, score)\n",
        "            print(label) \n",
        "        print(\"**************************\")\n",
        "        return out_classes\n",
        "    \n",
        "    def threaded(self,con):\n",
        "        \n",
        "        print(class_labels)\n",
        "        i=0;\n",
        "        #for label in class_labels:\n",
        "         #   con.send(json.dumps({\"label\":int(label)}).encode('utf-8'))\n",
        "        #if len(class_labels) == 0:\n",
        "            #con.send(json.dumps({\"label\":int(-1)}).encode('utf-8'))\n",
        "            #con.close()\n",
        "            #return \n",
        "        if len(class_labels) != 0:\n",
        "            con.send(json.dumps({\"label\":int(class_labels[0])}).encode('utf-8'))\n",
        "        con.close()\n",
        "\n",
        "    def close_session(self):\n",
        "        self.sess.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdn3i5c8MXW3"
      },
      "source": [
        "#### Reading video from a file by VideoCapture object\n",
        "Contains the video on which predictions should be made. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J1aQtDtyJCN"
      },
      "source": [
        "video = cv2.VideoCapture('yolo3/testvideo1.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906TsU4eMXW8"
      },
      "source": [
        "#### Creating Yolo Object "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK_LXXZmMXW-",
        "outputId": "c502030d-84ee-41ee-b805-4a5b8ddcb32a"
      },
      "source": [
        "yolo1 = YOLO()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:167: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:179: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:183: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1797: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1940: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J09XHoiMXXC"
      },
      "source": [
        "### Connection of Deep learning Model with Mobile Application\n",
        "Socket programming is used to connect the model with android application. UDP connection is made between the client and the server. Once the client i.e. RARroad application requests for the predictions, the model will send the predicted labels to it.\n",
        "\n",
        "The predictions returned can be either the 4 traffic signs we have included, other traffic signs or no sign ahead as the frame extracted from the video lacks any traffic sign."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpL5wdfB3RwK",
        "outputId": "2ea4b76b-eb3f-4744-90fa-af7d96cb1070"
      },
      "source": [
        "    \n",
        "\n",
        "port = 5000\n",
        "global class_labels\n",
        "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "s.bind((\"0.0.0.0\", port))\n",
        "s.listen(5)   \n",
        "\n",
        "# Variable for counting total amount of frames\n",
        "pre = -1\n",
        "# Catching frames in the loop\n",
        "while True:\n",
        "  # Capturing frames one-by-one\n",
        "    ret, frame = video.read()\n",
        "  # If the frame was not retrieved\n",
        "    if not ret:\n",
        "        break\n",
        "       \n",
        "    cv2.imwrite(\"frame.jpg\", frame)\n",
        "    \n",
        "    image = Image.open('frame.jpg')\n",
        "    class_labels = yolo1.detect_image()\n",
        "    \n",
        "    i = 0\n",
        "    while(i<13):\n",
        "        frame = video.read()\n",
        "        i+=1\n",
        "   \n",
        "    frame = None\n",
        "    #Now lets accept connections from devices and connect with them\n",
        "    if(len(class_labels) == 0):\n",
        "        continue\n",
        "    elif (pre == int(class_labels[0]) ):\n",
        "        continue\n",
        "        \n",
        "    print(\"Waiting for client connection\") \n",
        "    conn, addr = s.accept()\n",
        "    print(\"Connected to\",conn,\":\",addr)\n",
        "\n",
        "    start_new_thread(yolo1.threaded, (conn,))\n",
        "    pre = int(class_labels[0])\n",
        "    \n",
        "    \n",
        "s.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 1 boxes for img\n",
            "Stop Sign 0.50\n",
            "**************************\n",
            "Waiting for client connection\n",
            "Connected to <socket.socket fd=332, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.10.10', 5000), raddr=('192.168.10.32', 58416)> : ('192.168.10.32', 58416)\n",
            "[3]\n",
            "Found 1 boxes for img\n",
            "Stop Sign 0.66\n",
            "**************************\n",
            "Found 1 boxes for img\n",
            "Stop Sign 0.33\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 2 boxes for img\n",
            "Speed Sign 50 0.72\n",
            "Speed Sign 30 0.39\n",
            "**************************\n",
            "Waiting for client connection\n",
            "Connected to <socket.socket fd=464, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.10.10', 5000), raddr=('192.168.10.32', 58456)> : ('192.168.10.32', 58456)\n",
            "[0 1]\n",
            "Found 1 boxes for img\n",
            "Speed Sign 50 0.58\n",
            "**************************\n",
            "Waiting for client connection\n",
            "Connected to <socket.socket fd=464, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.10.10', 5000), raddr=('192.168.10.32', 58476)> : ('192.168.10.32', 58476)\n",
            "[1]\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 0 boxes for img\n",
            "**************************\n",
            "Found 1 boxes for img\n",
            "Other 0.40\n",
            "**************************\n",
            "Waiting for client connection\n",
            "Connected to <socket.socket fd=464, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.10.10', 5000), raddr=('192.168.10.32', 58418)> : ('192.168.10.32', 58418)\n",
            "[4]\n",
            "Found 0 boxes for img\n",
            "**************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0uL9ONRMXXF"
      },
      "source": [
        "#### Closing Yolo Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpQn4ukCMXXG"
      },
      "source": [
        "yolo1.close_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLB7CN7aMXXH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}